{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can install v1.15 tensorflow but we recommend running the notebook on colab as its current default is 1.x\n",
    "# Be careful with python versions, keras and tensorflow!\n",
    "#!pip install q tensorflow==2.2.4\n",
    "#!pip install q keras==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)]\n",
      "1.15.0\n",
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import virl\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "# Import the open AI gym\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "#from keras import backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "\n",
    "print(sys.version)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a few helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a couple of helper functions\n",
    "import pandas as pd\n",
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "def plot_episode_stats(stats, smoothing_window,environment,noisy, noshow=False):\n",
    "\n",
    "    # Plot the episode reward over time,so we can see when did it converge\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    plt.savefig('graphs/ps_tm/converge graph/average performance vs number of episodes'+ \"for ps_tm agent\" + \" \" + environment + \" \" + noisy + \" \"+ '.png')\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Policy Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizing(state):\n",
    "    bins = np.array([0e+8, 0.5e+8, 1e+8, 1.5e+8, 2e+8, 2.5e+8,3e+8,3.5e+8,4e+8,4.5e+8,5e+8,5.5e+8])\n",
    "    index = np.digitize(state, bins) - 1\n",
    "    return index[0] + index[1] * 12 + index[2] * 12 * 12 + index[3] * 12 * 12 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkPolicyEstimator():\n",
    "    \"\"\" \n",
    "    A very basic MLP neural network approximator and estimator for poliy search    \n",
    "    \n",
    "    The only tricky thing is the traning/loss function and the specific neural network arch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, n_actions, d_states, nn_config, verbose=False):        \n",
    "        self.alpha = alpha    \n",
    "        self.nn_config = nn_config                   \n",
    "        self.n_actions = n_actions        \n",
    "        self.d_states = d_states\n",
    "        self.verbose=verbose # Print debug information        \n",
    "        self.n_layers = len(nn_config)  # number of hidden layers        \n",
    "        self.model = []\n",
    "        self.__build_network(d_states, n_actions)\n",
    "        self.__build_train_fn()\n",
    "             \n",
    "\n",
    "    def __build_network(self, input_dim, output_dim):\n",
    "        \"\"\"Create a base network usig the Keras functional API\"\"\"\n",
    "        self.X = layers.Input(shape=(input_dim,))\n",
    "        net = self.X\n",
    "        for h_dim in self.nn_config:\n",
    "            net = layers.Dense(h_dim)(net)\n",
    "            net = layers.Activation(\"relu\")(net)\n",
    "        net = layers.Dense(output_dim, kernel_initializer=initializers.Zeros())(net)\n",
    "        net = layers.Activation(\"softmax\")(net)\n",
    "        self.model = Model(inputs=self.X, outputs=net)\n",
    "\n",
    "    def __build_train_fn(self):\n",
    "        \"\"\" Create a custom train function\n",
    "        It replaces `model.fit(X, y)` because we use the output of model and use it for training.        \n",
    "        Called using self.train_fn([state, action_one_hot, target])`\n",
    "        which would train the model. \n",
    "        \n",
    "        Hint: you can think of K. as np.\n",
    "        \n",
    "        \"\"\"\n",
    "        # predefine a few variables\n",
    "        action_onehot_placeholder   = K.placeholder(shape=(None, self.n_actions),name=\"action_onehot\") # define a variable\n",
    "        target                      = K.placeholder(shape=(None,), name=\"target\") # define a variable       \n",
    "        \n",
    "        # this part defines the loss and is very important!\n",
    "        action_prob        = self.model.output # the outlout of the neural network        \n",
    "        action_selected_prob        = K.sum(action_prob * action_onehot_placeholder, axis=1) # probability of the selcted action        \n",
    "        log_action_prob             = K.log(action_selected_prob) # take the log\n",
    "        loss = -log_action_prob * target # the loss we are trying to minimise\n",
    "        loss = K.mean(loss)\n",
    "        \n",
    "        # defining the speific optimizer to use\n",
    "        adam = optimizers.Adam(lr=self.alpha)# clipnorm=1000.0) # let's use a kereas optimiser called Adam\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights,loss=loss) # what gradient updates to we parse to Adam\n",
    "            \n",
    "        # create a handle to the optimiser function    \n",
    "        self.train_fn = K.function(inputs=[self.model.input,action_onehot_placeholder,target],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates) # return a function which, when called, takes a gradient step\n",
    "      \n",
    "    \n",
    "    def predict(self, s, a=None):              \n",
    "        if a==None:            \n",
    "            return self._predict_nn(s)\n",
    "        else:                        \n",
    "            return self._predict_nn(s)[a]\n",
    "        \n",
    "    def _predict_nn(self,state_hat):                          \n",
    "        \"\"\"\n",
    "        Implements a basic MLP with tanh activations except for the final layer (linear)               \n",
    "        \"\"\"                \n",
    "        x = self.model.predict(state_hat)                                                    \n",
    "        return x\n",
    "  \n",
    "    def update(self, states, actions, target):  \n",
    "        \"\"\"\n",
    "            states: a interger number repsenting the discrete state\n",
    "            actions: a interger number repsenting the discrete action\n",
    "            target: a real number representing the discount furture reward, reward to go\n",
    "            \n",
    "        \"\"\"\n",
    "        action_onehot = np_utils.to_categorical(actions, num_classes=self.n_actions) # encodes the state as one-hot\n",
    "        self.train_fn([states, action_onehot, target]) # call the custom optimiser which takes a gradient step\n",
    "        return \n",
    "        \n",
    "    def new_episode(self):        \n",
    "        self.t_episode  = 0.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Reinforce Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, estimator_policy, num_episodes, discount_factor=0.9):\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) Algorithm. Optimizes the policy\n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized         \n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: reward discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \n",
    "    Adapted from: https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            state_oh = np.zeros((1,12**4))\n",
    "            state_oh[0,quantizing(state)] = 1.0                                  \n",
    "            action_probs = estimator_policy.predict(state_oh)\n",
    "            action_probs = action_probs.squeeze()\n",
    "            \n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            \n",
    "            ##\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")            \n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "        # Go through the episode, step-by-step and make policy updates (note we sometime use j for the individual steps)\n",
    "        estimator_policy.new_episode()\n",
    "        new_theta=[]\n",
    "        for t, transition in enumerate(episode):                 \n",
    "            state_oh = np.zeros((1,12**4))\n",
    "            state_oh[0,quantizing(transition.state)] = 1.0 \n",
    "            \n",
    "            # The return, G_t, after this timestep; this is the target for the PolicyEstimator\n",
    "            G_t = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "           \n",
    "            # Update our policy estimator\n",
    "            estimator_policy.update(state_oh, transition.action,np.array(G_t))            \n",
    "         \n",
    "    return stats,estimator_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the traning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_action(actions):\n",
    "  outcome = \"\\n\"\n",
    "  for i in range (len(actions)):\n",
    "    outcome += (\"week \" + str('{:2}'.format(i+1)) + \" : \")\n",
    "    if actions[i] == 0:\n",
    "      outcome += (\" None              \")\n",
    "    if actions[i] == 1:\n",
    "      outcome += (\" Full Lockdown     \")\n",
    "    if actions[i] == 2:\n",
    "      outcome += (\" Track & Trace     \")\n",
    "    if actions[i] == 3:\n",
    "      outcome += (\" Social Distancing \")\n",
    "    if len(actions) - 1 != i:\n",
    "      outcome += (\" -> \")\n",
    "    if (i+1) % 4 == 0:\n",
    "      outcome += \"\\n\"\n",
    "  return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode(env,policy_estimator,environment,noisy):\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    states.append(s)\n",
    "    while not done:\n",
    "        state_oh = np.zeros((1,12**4))\n",
    "        state_oh[0,quantizing(s)] = 1.0                                  \n",
    "        action_probs = policy_estimator.predict(state_oh)\n",
    "        action_probs = action_probs.squeeze()\n",
    "\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    print(show_action(actions))\n",
    "    labels = ['s[0]: susceptibles', 's[1]: infectious', 's[2]: quarantined', 's[3]: recovereds']\n",
    "    states = np.array(states)\n",
    "    for i in range(4):\n",
    "        axes[0].set_title(\"The states analysis for\" + \" \" + environment + \" \" + noisy + \" \" + \"for ps_tm\")\n",
    "        axes[0].plot(states[:,i], label=labels[i]);\n",
    "        axes[0].set_xlabel('weeks since start of epidemic')\n",
    "        axes[0].set_ylabel('State s(t)')\n",
    "        axes[0].legend()\n",
    "        axes[1].plot(rewards);\n",
    "        axes[1].set_title('The Reward to each action' + \" \" + environment + \" \" + noisy + \" \" + \"for ps_tm\")\n",
    "        axes[1].set_xlabel('weeks since start of epidemic')\n",
    "        axes[1].set_ylabel('reward r(t)')\n",
    "    \n",
    "    print('total reward', np.sum(rewards))\n",
    "    plt.savefig('graphs/ps_tm/state_and_reward/state and reward for'+ \" \" + environment + \" \" + noisy + \" \" + \"for ps_tm\" + '.png')\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_search_train_and_predict(env,num_episodes,environment,noisy):\n",
    "    # Instantiate a FunctionApproximator (i.e. the linear function approximator)\n",
    "\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    #policy_estimator = trained_policy_estimator\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        alpha = 0.001  \n",
    "        n_actions = env.action_space.n\n",
    "        n_states = 12**4\n",
    "        nn_config = []\n",
    "\n",
    "        policy_estimator = NeuralNetworkPolicyEstimator(alpha, n_actions, n_states, nn_config)\n",
    "        stats,estimator = reinforce(env, policy_estimator,num_episodes, discount_factor=0.97)\n",
    "        ## then,we use the optimized function approximator to predict on optimal policy and gain them\n",
    "        actions = run_one_episode(env,estimator,environment,noisy)\n",
    "        #print(show_action(actions))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_training_and_evaluation(num_episodes):\n",
    "    \n",
    "    stats_for_problems_nnoise = []\n",
    "    for i in range (10):\n",
    "        env = virl.Epidemic(problem_id=i,noisy=False)\n",
    "        print(\"This is problem\" + \" \" + str(i) + \" \" + \"without noise for ps_tm\")\n",
    "        stats0 = policy_search_train_and_predict(env,num_episodes,\"problem\" + \" \" + str(i),\"without noise\")\n",
    "        stats_for_problems_nnoise.append(stats0)\n",
    "        \n",
    "    stats_for_problems_noise = []\n",
    "    for j in range (10):\n",
    "        env = virl.Epidemic(problem_id=j,noisy=True)\n",
    "        print(\"This is problem\" + \" \" + str(j) + \" \" + \"with noise for ps_tm\")\n",
    "        stats1 = policy_search_train_and_predict(env,num_episodes,\"problem\" + \" \" + str(j),\"with noise\")\n",
    "        stats_for_problems_noise.append(stats1)\n",
    "        \n",
    "\n",
    "    env = virl.Epidemic(stochastic=True,noisy=False)\n",
    "    print(\"This is stochastic problem \" +  \" \" + \"without noise for ps_tm\")\n",
    "    stats2 = policy_search_train_and_predict(env,num_episodes,\"stochastic\",\"without noise\")\n",
    "    \n",
    "    env = virl.Epidemic(stochastic=True,noisy=True)\n",
    "    print(\"This is stochastic problem \" +  \" \" + \"with noise for ps_tm\")\n",
    "    stats3 = policy_search_train_and_predict(env,num_episodes,\"stochastic\",\"with noise\")\n",
    "    \n",
    "    print(\"----------------------------------Learning Curves----------------------------------------------\")\n",
    "    return stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graphs_policy_pstm(stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3):\n",
    "    '''Used for ploting the converging curve for each training problem and environments'''\n",
    "    \n",
    "    for i in range(len(stats_for_problems_nnoise)):\n",
    "        plot_episode_stats(stats_for_problems_nnoise[i],100,\"problem\" + \" \" + str(i),\"without noise\")\n",
    "        print(\"This is the convergence graphs for problem\" + \" \" + str(i) + \" \" + \"without noise for ps_tm\")\n",
    "    for j in range(len(stats_for_problems_noise)):\n",
    "        plot_episode_stats(stats_for_problems_noise[j],100,\"problem\" + \" \" + str(j),\"with noise\")\n",
    "        print(\"This is the convergence graphs for problem\" + \" \" + str(j) + \" \" + \"with noise for ps_tm\")\n",
    "        \n",
    "    plot_episode_stats(stats2,100,\"stochastic\",\"without noise\")\n",
    "    print(\"This is the convergence graphs for stochastic environment\" +  \" \" + \"without noise for ps_tm\")\n",
    "    \n",
    "    plot_episode_stats(stats3,100,\"stochastic\",\"with noise\")\n",
    "    print(\"This is the convergence graphs for stochastic environment\" +  \" \" + \"with noise for ps_tm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_for_policySearch_tm():\n",
    "    # Train and evaluate as well as drawing state graph and reward graph\n",
    "    stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3 = overall_training_and_evaluation(2000)\n",
    "    # Plot the converging curve\n",
    "    show_graphs_policy_pstm(stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization_for_policySearch_tm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

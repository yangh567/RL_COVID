{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('..')\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "import virl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Help function to see the converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "# This is used to compare the episode length over time,which is always 52\n",
    "def plot_episode_stats(stats, smoothing_window,environment,noisy,noshow=False):\n",
    "\n",
    "    # Plot the episode reward over time,so we can see when did it converge\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    plt.savefig('graphs/random_agent/converge graph/average performance vs number of episodes'+ \"for random agent\" + \" \" + environment + \" \" + noisy + \" \"+ '.png')\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    #return fig1, fig2, fig3\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on the environment and doing fine tuning and predicting on the action to construct visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  First : make the function to show the action content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_action(actions):\n",
    "    outcome = \" \" \n",
    "    for i in range(len(actions)):\n",
    "        outcome += (\"week \" + str(\"{:2}\".format(i+1)) + \" : \")\n",
    "        if actions[i] == 0:\n",
    "            outcome += (\"%-17s\" %(\"None\"))\n",
    "        if actions[i] == 1:\n",
    "            outcome += (\"%-17s\" %(\"Full Lockdown\"))\n",
    "        if actions[i] == 2:\n",
    "            outcome += (\"%-17s\" %(\"Track & Trace\"))\n",
    "        if actions[i] == 3:\n",
    "            outcome += (\"%-17s\" %(\"Social Distancing\"))\n",
    "        if len(actions) - 1 != i:\n",
    "            outcome += (\" -> \")\n",
    "        if (i+1) % 4 == 0:\n",
    "            outcome += \"\\n\"\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second : make the function to run one episode with the trained deterministic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode(env,environment,noisy):\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    states.append(s)\n",
    "    while not done:\n",
    "        # make a step\n",
    "        action=np.random.choice(env.action_space.n)\n",
    "        next_state, reward, done, _ = env.step(action) # random agent\n",
    "\n",
    "        states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    # This is the place for visualizing the policy\n",
    "    print(\"\\n\")\n",
    "    print(show_action(actions))\n",
    "    labels = ['s[0]: susceptibles', 's[1]: infectious', 's[2]: quarantined', 's[3]: recovereds']\n",
    "    states = np.array(states)\n",
    "    for i in range(4):\n",
    "        axes[0].set_title(\"The states analysis for\" + \" \" + environment + \" \" + noisy + \" \" + \"for random agent\")\n",
    "        axes[0].plot(states[:,i], label=labels[i]);\n",
    "        axes[0].set_xlabel('weeks since start of epidemic')\n",
    "        axes[0].set_ylabel('State s(t)')\n",
    "        axes[0].legend()\n",
    "        axes[1].plot(rewards);\n",
    "        axes[1].set_title('The Reward to each action' + \" \" + environment + \" \" + noisy + \" \" + \"for random agent\")\n",
    "        axes[1].set_xlabel('weeks since start of epidemic')\n",
    "        axes[1].set_ylabel('reward r(t)')\n",
    "    \n",
    "    print('total reward', np.sum(rewards))\n",
    "    plt.savefig('graphs/random_agent/state_and_reward/state and reward for'+ \" \" + environment + \" \" + noisy + \" \" + \"for ps_lfa\" + '.png')\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third : make the function to train the agent and evaluate on the trained agent by making one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random(env,num_episodes):\n",
    "    \n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes)) \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        t = 0\n",
    "        while not done:\n",
    "            t += 1\n",
    "            s, r, done, info = env.step(action=np.random.choice(env.action_space.n)) # random agent\n",
    "            stats.episode_rewards[i_episode] += r\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")            \n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_train_and_predict(env,num_episodes,environment,noisy):\n",
    "    # Instantiate a FunctionApproximator (i.e. the linear function approximator)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        # This is the training stage\n",
    "        # policy may vary. ~600-1000 seemed to work well.\n",
    "        stats = run_random(env,num_episodes)\n",
    "        ## then,we use the optimized function approximator to predict on optimal policy and gain them\n",
    "        actions = run_one_episode(env,environment,noisy)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth : make the function to train the agent in all environment and problems and evaluate on the trained agent by making one episodeÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_training_and_evaluation(num_episodes):\n",
    "    \n",
    "    '''Train for each of the environments and problems'''\n",
    "    \n",
    "    stats_for_problems_nnoise = []\n",
    "    for i in range (10):\n",
    "        env = virl.Epidemic(problem_id=i,noisy=False)\n",
    "        print(\"This is problem\" + \" \" + str(i) + \" \" + \"without noise for random agent\")\n",
    "        # train on the problem and evaluating by making one episode\n",
    "        stats0 = random_train_and_predict(env,num_episodes,\"problem\" + \" \" + str(i),\"without noise\")\n",
    "        stats_for_problems_nnoise.append(stats0)\n",
    "        \n",
    "    stats_for_problems_noise = []\n",
    "    for j in range (10):\n",
    "        env = virl.Epidemic(problem_id=j,noisy=True)\n",
    "        # train on the problem and evaluating by making one episode\n",
    "        print(\"This is problem\" + \" \" + str(j) + \" \" + \"with noise for random agent\")\n",
    "        stats1 = random_train_and_predict(env,num_episodes,\"problem\" + \" \" + str(j),\"with noise\")\n",
    "        stats_for_problems_noise.append(stats1)\n",
    "        \n",
    "\n",
    "    env = virl.Epidemic(stochastic=True,noisy=False)\n",
    "    # train on the stochastic environment and evaluating by making one episode\n",
    "    print(\"This is stochastic problem \" +  \" \" + \"without noise for random agent\")\n",
    "    stats2 = random_train_and_predict(env,num_episodes,\"stochastic\",\"without noise\")\n",
    "    \n",
    "    env = virl.Epidemic(stochastic=True,noisy=True)\n",
    "    # train on the stochastic environment and evaluating by making one episode\n",
    "    print(\"This is stochastic problem \" +  \" \" + \"with noise for random agent\")\n",
    "    stats3 = random_train_and_predict(env,num_episodes,\"stochastic\",\"with noise\")\n",
    "    \n",
    "    print(\"----------------------------------Learning Curves----------------------------------------------\")\n",
    "    return stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fifth :Draw the graph to see how the agent learn (Learning curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graphs_policy_random(stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3):\n",
    "    '''Used for ploting the converging curve for each training problem and environments'''\n",
    "    \n",
    "    for i in range(len(stats_for_problems_nnoise)):\n",
    "        plot_episode_stats(stats_for_problems_nnoise[i],100,\"problem\" + \" \" + str(i),\"without noise\")\n",
    "        print(\"This is the convergence graphs for problem\" + \" \" + str(i) + \" \" + \"without noise for random agent\")\n",
    "    for j in range(len(stats_for_problems_noise)):\n",
    "        plot_episode_stats(stats_for_problems_noise[j],100,\"problem\" + \" \" + str(j),\"with noise\")\n",
    "        print(\"This is the convergence graphs for problem\" + \" \" + str(j) + \" \" + \"with noise for random agent\")\n",
    "        \n",
    "    plot_episode_stats(stats2,100,\"stochastic\",\"without noise\")\n",
    "    print(\"This is the convergence graphs for stochastic environment\" +  \" \" + \"without noise for random agent\")\n",
    "    \n",
    "    plot_episode_stats(stats3,100,\"stochastic\",\"with noise\")\n",
    "    print(\"This is the convergence graphs for stochastic environment\" +  \" \" + \"with noise for random agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The overall visualization for all of the environment and problems for passing to run_eval.ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_for_random_agent():\n",
    "    # Train and evaluate as well as drawing state graph and reward graph\n",
    "    stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3 = overall_training_and_evaluation(2000)\n",
    "    # Plot the converging curve\n",
    "    show_graphs_policy_random(stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualization_for_random_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

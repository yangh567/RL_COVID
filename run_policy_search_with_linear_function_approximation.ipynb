{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)]\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "##os.chdir('..')\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import virl\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "# Import the open AI gym\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "print(sys.version)\n",
    "print(tf.__version__)\n",
    "#from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Zhouyang Shen:Help function to see the converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "# This is used to compare the episode length over time,which is always 52\n",
    "def plot_episode_stats(stats, smoothing_window,environment,noisy, noshow=False):\n",
    "\n",
    "    # Plot the episode reward over time,so we can see when did it converge\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    plt.savefig('graphs/ps_lfa/converge graph/average performance vs number of episodes'+ \"for ps_lfa agent\" + \" \" + environment + \" \" + noisy + \" \"+ '.png')\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zhouyang Shen:Initialze Environment for observation purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.37812848, 1.54003528, 0.25362512, 0.07612969])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = virl.Epidemic(problem_id=0,noisy=False)\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zhouyang Shen:Draw state samples from the environment  and train the featurizer for function approximation on states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(env):\n",
    "    observation_examples = np.array([env.observation_space.sample() for x in range(2000000)])\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(observation_examples)\n",
    "    return scaler,observation_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Preprocessing: we normalize the data to zero mean and unit variance\n",
    "# We use samples (10000) from the observation space for normalization\n",
    "\n",
    "def get_featurizer(env,scaler,observation_samples):\n",
    "    \n",
    "    # Used to converte a state to a featurizes represenation for fuction approximation.\n",
    "    # We use RBF kernels with different variances to gather different parts of the space\n",
    "    featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=0.0, n_components=10)),\n",
    "        ])\n",
    "    ## train the featurizer here \n",
    "    featurizer.fit(scaler.transform(observation_samples))\n",
    "    return featurizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zhouyang Shen:Define the policy estimator (Function approximator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionApproximator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    taking learning rate \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, featurizer,scaler,learning_rate=0.01,scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.scaler = scaler\n",
    "            self.featurizer = featurizer\n",
    "\n",
    "            # Allocate the space space for each states\n",
    "            self.state = tf.placeholder(tf.float32, [10], \"state\")\n",
    "            # Allocate the action space for each step\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            # Initialize Goal(sj) for each state\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is a linear estimator take in:state;output:different h-value related to different actions\n",
    "            self.output = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=env.action_space.n,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            self.outcome = tf.nn.softmax(self.output)\n",
    "            # this will produce the policy(𝜋(𝑎𝑗|𝑠𝑗,𝜃)) we needed \n",
    "            self.action_probs = tf.squeeze(self.outcome)\n",
    "            # Loss (we take log of it)and train the function with loss = −𝑙𝑜𝑔(𝜋(𝑎𝑗|𝑠𝑗,𝜃))∗𝐺𝑡\n",
    "            self.loss = -tf.log(tf.gather(self.action_probs, self.action)) * self.target\n",
    "            # we use Adam optimizer with provided learning rate to train the function\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.training_operation = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "            \n",
    "    def featurize_state(self,state):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a state.\n",
    "        \"\"\"\n",
    "        scaled = self.scaler.transform([state])\n",
    "        featurized = self.featurizer.transform(scaled)\n",
    "        return featurized[0]\n",
    "\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        # First transform state to feature representation and then predict the 𝜋(𝑎𝑗|𝑠𝑗,𝜃) for state\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = self.featurize_state(state)        \n",
    "        return sess.run(self.action_probs, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "        # This will update the parameter by doing 𝜃←𝜃+𝛼𝐺𝑡∇𝜋(𝑎𝑗|𝑠𝑗,𝜃)𝜋(𝑎𝑗|𝑠𝑗,𝜃)\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: self.featurize_state(state), self.target: target, self.action: action  }\n",
    "        _, loss = sess.run([self.training_operation, self.loss], feed_dict)  # Call the custom optimiser which takes a gradient step\n",
    "        return loss\n",
    "    def new_episode(self):        \n",
    "        self.t_episode  = 0.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zhouyang Shen:Define the policy search algorithm(Reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, estimator, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    REINFORCE Algorithm. Optimizes the policy\n",
    "    function approximator using policy search.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized         \n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: reward discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps the track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    # save the transition for sampling at end\n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        done = False\n",
    "        # One step in the environment\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Take a step\n",
    "            action_probs = estimator.predict(state) # calculate 𝜋(𝑎𝑗|𝑠𝑗,𝜃)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs) # make action base on policy\n",
    "            ## getting to nest state\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")            \n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "        # Go through the episode, step-by-step and make policy updates\n",
    "        estimator.new_episode()\n",
    "        \n",
    "        for t, transition in enumerate(episode):                 \n",
    "            \n",
    "            # The return, G_t, after this timestep; this is the target for the FunctionApproximator\n",
    "            G_t = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "           \n",
    "            # Update our policy estimator\n",
    "            estimator.update(transition.state,np.array(G_t),transition.action)       \n",
    "         \n",
    "    return stats,estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zhouyang Shen:Train on the environment and doing fine tuning and predicting on the action to construct visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First : make the function to show the action content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_action(actions):\n",
    "    outcome = \"\\n\" \n",
    "    for i in range(len(actions)):\n",
    "        outcome += (\"week \" + str(\"{:2}\".format(i+1)) + \" : \")\n",
    "        if actions[i] == 0:\n",
    "            outcome += (\"%-17s\" %(\"None\"))\n",
    "        if actions[i] == 1:\n",
    "            outcome += (\"%-17s\" %(\"Full Lockdown\"))\n",
    "        if actions[i] == 2:\n",
    "            outcome += (\"%-17s\" %(\"Track & Trace\"))\n",
    "        if actions[i] == 3:\n",
    "            outcome += (\"%-17s\" %(\"Social Distancing\"))\n",
    "        if len(actions) - 1 != i:\n",
    "            outcome += (\" -> \")\n",
    "        if (i+1) % 4 == 0:\n",
    "            outcome += \"\\n\"\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second : make the function to run one episode with the trained policy estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode(env,policy_estimator,environment,noisy):\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    states.append(s)\n",
    "    while not done:\n",
    "        # make a step\n",
    "        action_probs = policy_estimator.predict(s) # calculate 𝜋(𝑎𝑗|𝑠𝑗,𝜃) \n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)# make action base on policy\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    # This is the place for visualizing the policy\n",
    "    print(\"\\n\")\n",
    "    print(show_action(actions))\n",
    "    labels = ['s[0]: susceptibles', 's[1]: infectious', 's[2]: quarantined', 's[3]: recovereds']\n",
    "    states = np.array(states)\n",
    "    for i in range(4):\n",
    "        axes[0].set_title(\"The states analysis for\" + \" \" + environment + \" \" + noisy + \" \" + \"for ps_lfa\")\n",
    "        axes[0].plot(states[:,i], label=labels[i]);\n",
    "        axes[0].set_xlabel('weeks since start of epidemic')\n",
    "        axes[0].set_ylabel('State s(t)')\n",
    "        axes[0].legend()\n",
    "        axes[1].plot(rewards);\n",
    "        axes[1].set_title('The Reward to each action' + \" \" + environment + \" \" + noisy + \" \" + \"for ps_lfa\")\n",
    "        axes[1].set_xlabel('weeks since start of epidemic')\n",
    "        axes[1].set_ylabel('reward r(t)')\n",
    "    \n",
    "    print('total reward', np.sum(rewards))\n",
    "    plt.savefig('graphs/ps_lfa/state_and_reward/state and reward for'+ \" \" + environment + \" \" + noisy + \" \" + \"for ps_lfa\" + '.png')\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third : make the function to train the agent and evaluate on the trained agent by making one episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_search_train_and_predict(env,num_episodes,environment,noisy):\n",
    "    # Instantiate a FunctionApproximator (i.e. the linear function approximator)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    scaler,observation_samples = get_scaler(env)\n",
    "    featurizer = get_featurizer(env,scaler,observation_samples)\n",
    "    policy_estimator = FunctionApproximator(featurizer,scaler,learning_rate=0.001)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        # This is the training stage\n",
    "        # policy may vary. ~600-1000 seemed to work well.\n",
    "        stats,estimator = reinforce(env, policy_estimator,num_episodes, discount_factor=0.96)\n",
    "        ## then,we use the optimized function approximator to predict on optimal policy and gain them\n",
    "        actions = run_one_episode(env,estimator,environment,noisy)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth : make the function to train the agent in all environment and problems and evaluate on the trained agent by making one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env0 = virl.Epidemic(problem_id=0,noisy=False)\n",
    "#policy_estimator,stats = policy_search_train(env,1000)\n",
    "\n",
    "def overall_training_and_evaluation(num_episodes):\n",
    "    \n",
    "    '''Train for each of the environments and problems'''\n",
    "    \n",
    "    stats_for_problems_nnoise = []\n",
    "    for i in range (10):\n",
    "        env = virl.Epidemic(problem_id=i,noisy=False)\n",
    "        print(\"This is problem\" + \" \" + str(i) + \" \" + \"without noise for ps_lfa\")\n",
    "        # train on the problem and evaluating by making one episode\n",
    "        stats0 = policy_search_train_and_predict(env,num_episodes,\"problem\" + \" \" + str(i),\"without noise\")\n",
    "        stats_for_problems_nnoise.append(stats0)\n",
    "        \n",
    "    stats_for_problems_noise = []\n",
    "    for j in range (10):\n",
    "        env = virl.Epidemic(problem_id=j,noisy=True)\n",
    "         #train on the problem and evaluating by making one episode\n",
    "        print(\"This is problem\" + \" \" + str(j) + \" \" + \"with noise for ps_lfa\")\n",
    "        stats1 = policy_search_train_and_predict(env,num_episodes,\"problem\" + \" \" + str(j),\"with noise\")\n",
    "        stats_for_problems_noise.append(stats1)\n",
    "        \n",
    "\n",
    "    env = virl.Epidemic(stochastic=True,noisy=False)\n",
    "    # train on the stochastic environment and evaluating by making one episode\n",
    "    print(\"This is stochastic problem \" +  \" \" + \"without noise for ps_lfa\")\n",
    "    stats2 = policy_search_train_and_predict(env,num_episodes,\"stochastic\",\"without noise\")\n",
    "    \n",
    "    env = virl.Epidemic(stochastic=True,noisy=True)\n",
    "    # train on the stochastic environment and evaluating by making one episode\n",
    "    print(\"This is stochastic problem \" +  \" \" + \"with noise for ps_lfa\")\n",
    "    stats3 = policy_search_train_and_predict(env,num_episodes,\"stochastic\",\"with noise\")\n",
    "    \n",
    "    print(\"----------------------------------Learning Curves----------------------------------------------\")\n",
    "    return stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zhouyang Shen:Draw the graph to see how the agent learn (Learning curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graphs_policy_psla(stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3):\n",
    "    '''Used for ploting the converging curve for each training problem and environments'''\n",
    "    \n",
    "    for i in range(len(stats_for_problems_nnoise)):\n",
    "        plot_episode_stats(stats_for_problems_nnoise[i],100,\"problem\" + \" \" + str(i),\"without noise\")\n",
    "        print(\"This is the convergence graphs for problem\" + \" \" + str(i) + \" \" + \"without noise for ps_lfa\")\n",
    "    for j in range(len(stats_for_problems_noise)):\n",
    "        plot_episode_stats(stats_for_problems_noise[j],100,\"problem\" + \" \" + str(j),\"with noise\")\n",
    "        print(\"This is the convergence graphs for problem\" + \" \" + str(j) + \" \" + \"with noise for ps_lfa\")\n",
    "        \n",
    "    plot_episode_stats(stats2,100,\"stochastic\",\"without noise\")\n",
    "    print(\"This is the convergence graphs for stochastic environment\" +  \" \" + \"without noise for ps_lfa\")\n",
    "    \n",
    "    plot_episode_stats(stats3,100,\"stochastic\",\"with noise\")\n",
    "    print(\"This is the convergence graphs for stochastic environment\" +  \" \" + \"with noise for ps_lfa\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zhouyang Shen : The overall visualization for all of the environment and problems for passing to run_eval.ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_for_policySearch_lfa():\n",
    "    # Train and evaluate as well as drawing state graph and reward graph\n",
    "    stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3 = overall_training_and_evaluation(2000)\n",
    "    # Plot the converging curve\n",
    "    show_graphs_policy_psla(stats_for_problems_nnoise,stats_for_problems_noise,stats2,stats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##visualization_for_policySearch_lfa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
